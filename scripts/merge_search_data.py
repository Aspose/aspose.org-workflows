import json
import requests
from pathlib import Path
import sys

# Define languages and their codes
languages = {
    "ar": "Arabic", "cs": "Czech", "de": "German", "el": "Greek",
    "es": "Spanish", "fa": "Persian", "fr": "French", "id": "Indonesian",
    "it": "Italian", "ja": "Japanese", "ko": "Korean", "pl": "Polish",
    "pt": "Portuguese", "ru": "Russian", "sv": "Swedish", "th": "Thai",
    "tr": "Turkish", "uk": "Ukrainian", "vi": "Vietnamese", "zh": "Chinese"
}

# Define families
families = [
    "words", "pdf", "cells", "email", "slides", "imaging", "barcode", "diagram",
    "tasks", "ocr", "omr", "note", "cad", "3d", "html", "gis", "zip", "page", 
    "psd", "pub", "svg", "finance", "drawing", "font", "tex", "medical"
]

def get_hugo_public_folder():
    """Locate the public folder generated by Hugo."""
    public_folder = Path("./public")
    if not public_folder.exists():
        raise FileNotFoundError("Hugo 'public' folder not found. Ensure Hugo build was completed.")
    return public_folder

def merge_search_data(base_url):
    """Merge search data from multiple language files using the specified base URL."""
    public_folder = get_hugo_public_folder()
    
    # Iterate through each language and compile data
    for lang_code in languages.keys():
        merged_data = {}  # Initialize an empty dictionary for merging data

        # Loop through each family and attempt to merge search data
        for family in families:
            file_url = f"{base_url.rstrip('/')}/{family}/{lang_code}.search-data.json"
            
            try:
                response = requests.get(file_url)
                if response.status_code == 200:
                    data = response.json()
                    
                    # Merge all content directly without overwriting keys
                    for page_path, content in data.items():
                        if page_path not in merged_data:
                            merged_data[page_path] = content
                        else:
                            existing_content = merged_data[page_path]
                            
                            # Merge 'data' field if both have it
                            if "data" in content and "data" in existing_content:
                                existing_content["data"].update(content["data"])
                            elif "data" in content:
                                existing_content["data"] = content["data"]

                            # Merge other fields safely
                            for key, value in content.items():
                                if key != "data":
                                    existing_content[key] = value
                else:
                    print(f"File not found: {file_url}")

            except requests.exceptions.RequestException as e:
                print(f"Error fetching {file_url}: {e}")
            except json.JSONDecodeError as e:
                print(f"Invalid JSON in {file_url}: {e}")

        # Save the merged data into the existing public folder
        if merged_data:
            output_file = public_folder / f"{lang_code}.search-data.json"
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as output:
                json.dump(merged_data, output, ensure_ascii=False, indent=2)
            print(f"Merged data for {lang_code} saved to {output_file}")
        else:
            print(f"No valid data found for language: {lang_code}")

# Execute the merging function with CLI support
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python merge_search_data.py <base_url>")
        sys.exit(1)

    base_url = sys.argv[1]
    merge_search_data(base_url)
